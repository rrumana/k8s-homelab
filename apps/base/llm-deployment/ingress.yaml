apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama-ui
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    haproxy-ingress.github.io/backend-protocol: "http"
    haproxy-ingress.github.io/timeout-client: "3600s"
    haproxy-ingress.github.io/timeout-server: "3600s"
    haproxy-ingress.github.io/proxy-body-size: "0"
    haproxy-ingress.github.io/whitelist-source-range: "192.168.0.0/16,172.16.0.0/12,10.0.0.0/8"
spec:
  ingressClassName: haproxy-restricted
  tls:
    - hosts: ["llama.rcrumana.xyz"]
      secretName: llama-ui-tls
  rules:
    - host: llama.rcrumana.xyz
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: librechat
                port: { number: 3080 }

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: llama-api
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    haproxy-ingress.github.io/backend-protocol: "http"
    haproxy-ingress.github.io/timeout-client: "3600s"
    haproxy-ingress.github.io/timeout-server: "3600s"
    haproxy-ingress.github.io/proxy-body-size: "0"
    haproxy-ingress.github.io/whitelist-source-range: "192.168.0.0/16,172.16.0.0/12,10.0.0.0/8"
spec:
  ingressClassName: haproxy-restricted
  tls:
    - hosts: ["llama-api.rcrumana.xyz"]
      secretName: llama-api-tls
  rules:
    - host: llama-api.rcrumana.xyz
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: llama-api
                port: { number: 8080 }
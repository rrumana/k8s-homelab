apiVersion: v1
kind: ConfigMap
metadata:
  name: librechat-config
  labels: { app: llama, component: librechat }
data:
  librechat.yaml: |
    version: 1.2.9
    cache: true
    endpoints:
      custom:
        - name: "LlamaCPP"
          apiKey: "${LLAMA_API_KEY}"
          baseURL: "https://llama-api.rcrumana.xyz/v1"
          models:
            fetch: true
          titleConvo: true
          titleModel: "qwen3-30b-thinking"
          summarize: false
          summaryModel: "qwen3-30b-thinking"
          forcePrompt: false
          modelDisplayLabel: "Llama.cpp"
    interface:
      defaultEndpoint: "LlamaCPP"
      defaultModel: "qwen3-30b-thinking"
      modelSelect: true
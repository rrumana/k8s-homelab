apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-swap-config
  labels: { app: llama, component: server }
data:
  config.yaml: |
    listen: ":8080"
    healthCheckTimeout: 900
    models:
      "qwen3-30b-thinking":
        # Unload the model if there's no traffic for 5 minutes
        ttl: 300
        useModelName: "qwen3-30b-thinking"
        cmd: |
          /app/llama-server
          -m /models/qwen3-30b-thinking.gguf
          -a qwen3-30b-thinking
          -c 16384
          -t 12
          -ngl 999
          -np 1
          --port ${PORT}
          --host 0.0.0.0
      "gpt-oss-20b":
        # Unload the model if there's no traffic for 5 minutes
        ttl: 300
        useModelName: "gpt-oss-20b"
        cmd: |
          /app/llama-server
          -m /models/gpt-oss-20b.gguf
          -a gpt-oss-20b
          -c 16384
          -t 12
          -ngl 999
          -np 1
          --port ${PORT}
          --host 0.0.0.0
      "gemma3-1b":
        # Unload the model if there's no traffic for 5 minutes
        ttl: 300
        useModelName: "gemma3-1b"
        cmd: |
          /app/llama-server
          -m /models/gemma-3-4b.gguf
          -a gemma3-1b
          -c 16384
          -t 12
          -ngl 999
          -np 1
          --port ${PORT}
          --host 0.0.0.0

    # Later: you can also keep several models resident at once with "groups" if desired.
    # groups:
    #   large-only-one:
    #     swap: true   # only one model in this group runs at a time
    #     members: ["qwen3-30b-thinking"]

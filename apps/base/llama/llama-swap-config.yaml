apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-swap-config
  labels: { app: llama, component: server }
data:
  config.yaml: |
    # llama-swap listens where your Service expects it
    listen: ":8080"

    # Give large models time to warm up; tune as you like
    healthCheckTimeout: 900   # seconds to wait for llama-server to become healthy

    # One-model-on-demand. Add more entries under "models:" to expose multiple choices.
    models:
      "qwen3-30b-thinking":
        # Unload the model if there's no traffic for 30 minutes
        ttl: 1800
        # Optional: if you want the OpenAI "model" name sent to upstream to be different
        useModelName: "qwen3-30b-thinking"

        # Exact command llama-swap will run to serve requests for this model.
        # NOTE: ${PORT} is injected by llama-swap so multiple models can reuse the same template.
        cmd: |
          /app/llama-server
          -m /models/primary.gguf
          -a qwen3-30b-thinking
          -c 16384
          -t 12
          -ngl 999
          -np 1
          --port ${PORT}
          --host 0.0.0.0

    # Later: you can also keep several models resident at once with "groups" if desired.
    # groups:
    #   large-only-one:
    #     swap: true   # only one model in this group runs at a time
    #     members: ["qwen3-30b-thinking"]

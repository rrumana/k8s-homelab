apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: llama-server
  labels:
    app: llama
    component: server
spec:
  serviceName: llama-api
  replicas: 1                     # set to the number of nodes you want to serve on (1..N)
  selector:
    matchLabels:
      app: llama
      component: server
  template:
    metadata:
      labels:
        app: llama
        component: server
    spec:
      # Only schedule on nodes you approve for serving:
      #   kubectl label node miniserver llama.enabled=true
      #   kubectl label node worker-1 llama.enabled=true
      nodeSelector:
        llama.enabled: "true"

      # Keep one replica per node
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: llama
                  component: server
              topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: llama
              component: server

      containers:
        - name: llama-server
          # For AMD iGPU, Vulkan build is a good default. Switch to :server-rocm if you prefer ROCm.
          image: ghcr.io/ggml-org/llama.cpp:server-vulkan
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: http
          env:
            - name: MODEL_PATH
              value: "/models/primary.gguf"
            - name: CONTEXT
              value: "8192"
            - name: THREADS
              value: "12"
            - name: CONCURRENCY
              value: "4"
            - name: N_GPU_LAYERS
              value: "999"
            - name: HSA_OVERRIDE_GFX_VERSION
              value: "11.5.1"
          command: ["/bin/sh", "-lc"]
          args:
            - >
              exec llama-server
              -m "$MODEL_PATH"
              -c $(CONTEXT)
              -np $(CONCURRENCY)
              --port 8080 --host 0.0.0.0
          volumeMounts:
            - name: models
              mountPath: /models
            - name: dri
              mountPath: /dev/dri
            - name: kfd
              mountPath: /dev/kfd
          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "24"
              memory: "48Gi"
          securityContext:
            privileged: true
            capabilities:
              add: ["SYS_ADMIN"]
          readinessProbe:
            tcpSocket:
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 10

      initContainers:
        - name: seed-models
          image: alpine:3.20
          command: ["/bin/sh", "-lc"]
          args:
            - >
              set -euo pipefail;
              apk add --no-cache rsync coreutils;
              mkdir -p /models;
              rsync -a --delete /models-bus/ /models/ || true;
              if [ ! -f /models/primary.gguf ]; then
                set +e; FIRST=$(find /models -maxdepth 2 -type f -name '*.gguf' | head -n1); set -e;
                [ -n "$FIRST" ] && ln -sf "$FIRST" /models/primary.gguf || true;
              fi
          volumeMounts:
            - name: models-bus
              mountPath: /models-bus
              readOnly: true
            - name: models
              mountPath: /models

      volumes:
        - name: models-bus
          persistentVolumeClaim:
            claimName: llama-models-bus
        - name: dri
          hostPath:
            path: /dev/dri
            type: Directory
        - name: kfd
          hostPath:
            path: /dev/kfd
            type: CharDevice

  volumeClaimTemplates:
    - metadata:
        name: models
        labels:
          recurring-job.longhorn.io/source: enabled
          recurring-job-group.longhorn.io/default: enabled
      spec:
        storageClassName: longhorn-local-1r
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 128Gi
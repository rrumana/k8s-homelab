apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: llama-server
  labels:
    app: llama
    component: server
spec:
  serviceName: llama-api
  replicas: 1
  selector:
    matchLabels:
      app: llama
      component: server
  template:
    metadata:
      labels:
        app: llama
        component: server
    spec:
      # Schedule onto nodes you've approved for serving
      nodeSelector:
        llama.enabled: "true"

      # One replica per node (when you scale up)
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: llama
                  component: server
              topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: llama
              component: server

      # Seed node-local RWO cache from RWX "bus" before starting
      initContainers:
        - name: require-model-on-bus
          image: alpine:3.20
          command: ["sh","-lc"]
          args:
            - |
              set -e
              if ! find /models-bus -maxdepth 2 -type f -name '*.gguf' | head -n1 | grep -q . ; then
                echo "No .gguf files on bus PVC; seed a model first"; exit 1
              fi
          volumeMounts:
            - { name: models-bus, mountPath: /models-bus, readOnly: true }

        - name: seed-models
          image: alpine:3.20
          command: ["sh","-lc"]
          args:
            - |
              set -euo pipefail
              apk add --no-cache rsync findutils
              mkdir -p /models
              # Copy models from RWX bus to this node's local RWO
              rsync -a --delete /models-bus/ /models/ || true
              # Keep a stable symlink for the default model
              [ -f /models/primary.gguf ] || {
                FIRST="$(find /models -maxdepth 2 -type f -name '*.gguf' | head -n1 || true)"
                [ -n "$FIRST" ] && ln -sf "$FIRST" /models/primary.gguf
              }
          volumeMounts:
            - { name: models-bus, mountPath: /models-bus, readOnly: true }
            - { name: models,     mountPath: /models }

      containers:
        - name: llama-swap
          image: ghcr.io/mostlygeek/llama-swap:vulkan
          imagePullPolicy: IfNotPresent
          args: ["--config","/app/config.yaml","--listen",":8080"]
          ports:
            - { containerPort: 8080, name: http }
          # llama-server runs as a child process and needs GPU access
          securityContext:
            privileged: true
            capabilities: { add: ["SYS_ADMIN"] }
          volumeMounts:
            - { name: swap-config, mountPath: /app/config.yaml, subPath: config.yaml }
            - { name: models,      mountPath: /models }
            - { name: dri,         mountPath: /dev/dri }
            - { name: kfd,         mountPath: /dev/kfd }
          resources:
            requests: { cpu: "2",  memory: "4Gi" }
            limits:   { cpu: "24", memory: "48Gi" }
          startupProbe:
            httpGet: { path: /health, port: http }
            initialDelaySeconds: 5
            periodSeconds: 5
            failureThreshold: 120
          readinessProbe:
            httpGet: { path: /health, port: http }
            initialDelaySeconds: 5
            periodSeconds: 10

      volumes:
        - name: swap-config
          configMap:
            name: llama-swap-config
            items:
              - { key: config.yaml, path: config.yaml }
        - name: models-bus
          persistentVolumeClaim:
            claimName: llama-models-bus
        - name: dri
          hostPath: { path: /dev/dri, type: Directory }
        - name: kfd
          hostPath: { path: /dev/kfd, type: CharDevice }

  volumeClaimTemplates:
    - metadata:
        name: models
        labels:
          recurring-job.longhorn.io/source: enabled
          recurring-job-group.longhorn.io/default: enabled
      spec:
        storageClassName: longhorn-local-1r
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 128Gi
